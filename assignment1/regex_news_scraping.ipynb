{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e00158",
   "metadata": {},
   "source": [
    "# Assignment 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ef3a66",
   "metadata": {},
   "source": [
    "### Exercise 1.1 - News Article Scraping and Cleaning (20)\n",
    "\n",
    "Scrape 50 different news articles from various news websites and perform the following steps using basic python and regular expressions:\n",
    "\n",
    "1. Write a regular expression to extract all the URLs present in the news articles.\n",
    "2. How would you use regular expressions to extract the publication dates of the news articles?\n",
    "3. Create a regex pattern to extract all the author names mentioned in the articles.\n",
    "4. Explain how you would use regular expressions to extract email addresses of the authors from the articles.\n",
    "5. Write a regex pattern to identify and extract all the phone numbers mentioned in the news articles.\n",
    "6. How would you clean the text to remove HTML tags and special characters using regular expressions?\n",
    "7. Write a regular expression to identify and extract all mentions of organizations or companies in the articles.\n",
    "8. What approach would you take to clean the text and remove unnecessary whitespace and line breaks using regular expressions?\n",
    "9. Explain how you would use regular expressions to identify and extract all the headlines or titles of the news articles.\n",
    "10. Write a regex pattern to detect and extract all the mentions of important events or incidents discussed in the articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4ae40a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "711a4776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import random\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec339d",
   "metadata": {},
   "source": [
    "## STEP 1 - Selenium Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f6e4ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 60 URLs from 'news_urls_updated.csv'.\n",
      "Selenium WebDriver initiated...\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/c2056729058o?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/c93xprvdy23o?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/crkldd02xg8o?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/c62e3pny6p7o?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/c9865l8540eo?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/ckg1jkl09xeo?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/cqjwlxjn1xgo?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/clykwd9e256o?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/c0rpwk51qxro?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/cq50dx1jdnwo?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/ckgk21nng0vo?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/cx27yze8p0jo?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/cly4gwr20wzo?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/cvgqx7ygq41o?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/cr5qygr6d5yo?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/ckgywnjkrlqo?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/cj9zmeerp1xo?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/cqjevxvxw9xo?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/cpvl9k4mw8no?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.bbc.com/news/articles/c4g92xq8wdlo?at_medium=RSS&at_campaign=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/news/2025/10/14/russia-strikes-kharkiv-hospital-un-convoy-as-ukraine-seeks-us-tomahawks?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/economy/2025/10/14/imf-says-ai-investment-bubble-could-burst-comparable-to-dot-com-bubble?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/news/2025/10/14/mapping-the-rise-in-israeli-settler-attacks-across-the-occupied-west-bank?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/video/newsfeed/2025/10/14/gaza-journalist-buried-as-brother-is-freed-from-israeli-prison?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/news/2025/10/14/gaza-ceasefire-tested-as-israeli-forces-kill-five-palestinians?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/news/longform/2025/10/14/they-could-have-killed-me-protesters-condemn-state-violence-in-argentina?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/news/2025/10/14/who-is-in-charge-of-madagascar-after-president-rajoelina-flees?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/video/newsfeed/2025/10/14/colonel-says-military-has-taken-charge-in-madagascar?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/video/newsfeed/2025/10/14/freed-palestinian-detainees-allege-torture-in-israeli-jail?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/news/2025/10/14/why-has-dutch-government-taken-control-of-china-owned-chipmaker-nexperia?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/video/newsfeed/2025/10/14/deadly-storm-batters-alaska-leaving-thousands-displaced?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/news/2025/10/14/madagascar-president-dissolves-parliament-after-fleeing-army-backed-protest?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/video/newsfeed/2025/10/14/video-opposition-candidate-declares-victory-in-cameroon-election?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/sports/liveblog/2025/10/14/live-nigeria-vs-benin-caf-world-cup-qualifierfollo?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/video/between-us/2025/10/14/staying-in-gazas-north?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/opinions/2025/10/14/the-assassination-of-saleh-aljafarawi-is-meant-to-send-a-dark-message?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/video/newsfeed/2025/10/14/palestinians-return-to-the-rubble-of-their-homes-in-northern-gaza?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/news/2025/10/14/torrential-rains-collapse-venezuelan-gold-mine-killing-14?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/news/2025/10/14/russia-charges-exiled-oligarch-khodorkovsky-with-terrorism?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.aljazeera.com/video/newsfeed/2025/10/14/trump-declares-peace-but-sidesteps-two-state-solution-for-palestinians?traffic_source=rss...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/business/live-news/fox-news-dominion-trial-04-18-23/index.html...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/business/live-news/fox-news-dominion-trial-04-18-23/h_8d51e3ae2714edaa0dace837305d03b8...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/2023/04/17/media/dominion-fox-news-allegations/index.html...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/2023/04/18/media/fox-dominion-settlement/index.html...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/videos/politics/2023/04/18/jake-tapper-dominion-lawsuit-settlement-fox-news-statement-lead-vpx.cnn...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/2023/04/18/politics/mccarthy-biden-debt-ceiling/index.html...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/2023/04/18/us/kansas-city-ralph-yarl-shooting-tuesday/index.html...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/videos/us/2023/04/18/jeremy-renner-snowplow-accident-bodycam-nc-melas-contd-vpx.cnn...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/2023/04/18/entertainment/jake-gyllenhaal-jamie-lee-curtis-pandemic-living/index.html...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/2023/04/18/politics/white-house-toddler/index.html...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/2023/04/17/entertainment/jamie-foxx-remains-hospitalized/index.html...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/2023/04/18/us/benadryl-tiktok-challenge-teen-death-wellness/index.html...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/videos/us/2023/04/18/pizza-guy-trips-perp-moos-cprog-orig-bdk.cnn...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/2023/04/18/media/netflix-dvd-red-envelopes/index.html...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/2023/04/18/tech/lina-khan-ai-warning/index.html...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/2023/04/17/health/rise-type-2-diabetes-global-wellness/index.html...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/2023/04/18/health/teen-misuse-adhd-meds-wellness/index.html...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/videos/tech/2023/04/18/apple-store-mumbai-india-ceo-tim-cook-vedika-sud-ovn-biz-ldn-vpx.cnn...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/2023/04/18/politics/clarence-thomas-ethics-democrats/index.html...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Navigating to https://www.cnn.com/europe/live-news/russia-ukraine-war-news-04-18-23/index.html...\n",
      "Note: No common ad/popup element found or closed.\n",
      "Page loaded. Extracting HTML...\n",
      "Successfully processed and added data for the article.\n",
      "\n",
      "Browser closed.\n",
      "Saving all collected data to scraped_articles_final.json...\n",
      "All done. Data saved to scraped_articles_final.json\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "CSV_INPUT_FILENAME = 'news_urls_updated.csv'\n",
    "JSON_OUTPUT_FILENAME = 'scraped_articles_final.json'\n",
    "# --- Setup Parameters ---\n",
    "WAIT_TIME = 5 # seconds to wait for initial page load\n",
    "AD_WAIT_TIME = 3 # seconds to specifically wait for the ad/popup to appear\n",
    "\n",
    "# 1. FUNCTION TO READ URLS FROM CSV FILE (No Change)\n",
    "def get_urls_from_csv(filename):\n",
    "    \"\"\"Reads the 'URL' column from a CSV file and returns a list of URLs.\"\"\"\n",
    "    urls = []\n",
    "    \n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"ERROR: Input file not found at '{filename}'.\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(filename, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            header = next(reader)\n",
    "            \n",
    "            try:\n",
    "                url_index = header.index('URL')\n",
    "            except ValueError:\n",
    "                print(\"ERROR: 'URL' column not found in the CSV header.\")\n",
    "                return []\n",
    "            \n",
    "            for row in reader:\n",
    "                if len(row) > url_index:\n",
    "                    urls.append(row[url_index])\n",
    "        \n",
    "        print(f\"Successfully loaded {len(urls)} URLs from '{filename}'.\")\n",
    "        return urls\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Get the dynamic list of URLs ---\n",
    "urls_to_scrape = get_urls_from_csv(CSV_INPUT_FILENAME)\n",
    "\n",
    "if not urls_to_scrape:\n",
    "    print(\"No URLs to scrape. Exiting script.\")\n",
    "    exit()\n",
    "\n",
    "# --- This list will store the data for each article ---\n",
    "scraped_articles_data = []\n",
    "\n",
    "# --- Setup Selenium WebDriver ---\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "print(\"Selenium WebDriver initiated...\")\n",
    "\n",
    "# Loop through each URL\n",
    "for url in urls_to_scrape:\n",
    "    try:\n",
    "        print(f\"\\nNavigating to {url}...\")\n",
    "        driver.get(url)\n",
    "        time.sleep(WAIT_TIME)\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # 2. AD HANDLING LOGIC (METHOD 1)\n",
    "        # ----------------------------------------------------\n",
    "        # We define a common set of XPATHs/CSS selectors for popups/ads.\n",
    "        # This will try to find and click any of the following elements that appear.\n",
    "        AD_LOCATORS = [\n",
    "            (By.XPATH, \"//button[contains(text(), 'Close') or contains(text(), 'No thanks') or contains(text(), 'Skip')]\"),\n",
    "            (By.CSS_SELECTOR, \"div.ad-close, button.close-ad, button[aria-label='Close']\"),\n",
    "            (By.XPATH, \"//a[contains(@class, 'close') or contains(@class, 'ad-dismiss')]\")\n",
    "        ]\n",
    "\n",
    "        ad_closed = False\n",
    "        for locator_type, locator_value in AD_LOCATORS:\n",
    "            try:\n",
    "                # Wait for up to AD_WAIT_TIME seconds for the element to be clickable\n",
    "                close_button = WebDriverWait(driver, AD_WAIT_TIME).until(\n",
    "                    EC.element_to_be_clickable((locator_type, locator_value))\n",
    "                )\n",
    "                \n",
    "                # Execute the click and break the loop if successful\n",
    "                close_button.click()\n",
    "                print(f\"âœ¨ Ad closed using: {locator_value}\")\n",
    "                time.sleep(1) # Give the page a moment to settle\n",
    "                ad_closed = True\n",
    "                break\n",
    "                \n",
    "            except Exception:\n",
    "                # If a specific locator fails, just move on to the next one\n",
    "                continue\n",
    "        \n",
    "        if not ad_closed:\n",
    "            print(\"Note: No common ad/popup element found or closed.\")\n",
    "            \n",
    "        # ----------------------------------------------------\n",
    "        # END OF AD HANDLING\n",
    "        # ----------------------------------------------------\n",
    "\n",
    "        print(\"Page loaded. Extracting HTML...\")\n",
    "        final_html = driver.page_source\n",
    "\n",
    "        article_data = {\n",
    "            \"source_url\": url,\n",
    "            \"html_content\": final_html\n",
    "        }\n",
    "\n",
    "        scraped_articles_data.append(article_data)\n",
    "        print(f\"Successfully processed and added data for the article.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {url}. Error: {e}\")\n",
    "\n",
    "# --- Close the browser ---\n",
    "driver.quit()\n",
    "print(\"\\nBrowser closed.\")\n",
    "\n",
    "# --- Save the collected data to a single JSON file ---\n",
    "print(f\"Saving all collected data to {JSON_OUTPUT_FILENAME}...\")\n",
    "\n",
    "with open(JSON_OUTPUT_FILENAME, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(scraped_articles_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"All done. Data saved to {JSON_OUTPUT_FILENAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d09f399",
   "metadata": {},
   "source": [
    "##### This cell performs the web scraping using Selenium. First, it reads target news article URLs from `news_urls_updated.csv`. It then sets up and launches an automated Chrome browser instance. For each URL, it navigates to the page, waits briefly for loading, and attempts to close common ad popups by searching for predefined button/link elements. After handling potential ads, it captures the full HTML source code of the loaded page. Finally, it stores each URL and its corresponding HTML content in a list and saves this entire list as a JSON file named `scraped_articles_final.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9422b1",
   "metadata": {},
   "source": [
    "## STEP 2 - Strip JavaScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dd472ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Script/Noscript/Comment Stripping ---\n",
      "Successfully loaded 60 articles from scraped_articles_final.json.\n",
      "\n",
      "--- Script/Noscript/Comment Stripping Complete ---\n",
      "Successfully processed 60 articles.\n",
      "Saved results to scraped_articles_NO_SCRIPTS.json\n",
      "\n",
      "--- Sample Output after Stripping (First 500 chars of meaningful content) ---\n",
      "'<html lang=\"en-GB\"><head><meta charset=\"utf-8\"><meta name=\"viewport\" content=\"width=device-width\"><title>Son of hostage Amiram Cooper, whose body remains in Gaza, says \\'it\\'s not over\\'</title><meta name=\"page.section\" content=\"News\"><meta name=\"page.subsection\" content=\"Middle_east\"><meta property=\"og:title\" content=\"Son of hostage Amiram Cooper, whose body remains in Gaza, says \\'it\\'s not over\\'\"><meta name=\"twitter:title\" content=\"Son of hostage Amiram Cooper, whose body remains in Gaza, says \\'it'...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "RAW_INPUT_FILENAME = 'scraped_articles_final.json' # Input from Cell 2\n",
    "NO_SCRIPTS_OUTPUT_FILENAME = 'scraped_articles_NO_SCRIPTS.json' # Output for Cell 4\n",
    "\n",
    "# --- Enhanced Removal Function (Using '' replacement) ---\n",
    "def strip_unwanted_blocks(html_content):\n",
    "    \"\"\"Removes content within <script>, <noscript>, blocks,\n",
    "       and inline event handlers, replacing with empty strings.\"\"\"\n",
    "    if html_content is None: return \"\"\n",
    "\n",
    "    cleaned_html = html_content\n",
    "\n",
    "    # 1. Remove standard <script> blocks\n",
    "    SCRIPT_PATTERN = r'<script\\b[^>]*>.*?</script>'\n",
    "    cleaned_html = re.sub(SCRIPT_PATTERN, '', cleaned_html, flags=re.DOTALL | re.IGNORECASE) # Use ''\n",
    "\n",
    "    # 2. Remove <noscript> blocks\n",
    "    NOSCRIPT_PATTERN = r'<noscript\\b[^>]*>.*?</noscript>'\n",
    "    cleaned_html = re.sub(NOSCRIPT_PATTERN, '', cleaned_html, flags=re.DOTALL | re.IGNORECASE) # Use ''\n",
    "\n",
    "    # 3. Remove HTML comments\n",
    "    HTML_COMMENT_PATTERN = r''\n",
    "    cleaned_html = re.sub(HTML_COMMENT_PATTERN, '', cleaned_html, flags=re.DOTALL) # Use ''\n",
    "\n",
    "    # 4. Remove inline event handlers (like onclick=\"...\")\n",
    "    INLINE_EVENT_PATTERN = r'\\s+on\\w+\\s*=\\s*(?:\"[^\"]*\"|\\'[^\\']*\\')'\n",
    "    cleaned_html = re.sub(INLINE_EVENT_PATTERN, '', cleaned_html, flags=re.IGNORECASE) # Use ''\n",
    "\n",
    "    return cleaned_html\n",
    "\n",
    "# --- Main Processing Logic ---\n",
    "print(f\"--- Starting Script/Noscript/Comment Stripping ---\")\n",
    "try:\n",
    "    if not os.path.exists(RAW_INPUT_FILENAME):\n",
    "        print(f\"ERROR: File '{RAW_INPUT_FILENAME}' not found. Run Cell 2 first.\")\n",
    "    else:\n",
    "        with open(RAW_INPUT_FILENAME, 'r', encoding='utf-8') as f:\n",
    "            raw_data = json.load(f)\n",
    "        print(f\"Successfully loaded {len(raw_data)} articles from {RAW_INPUT_FILENAME}.\")\n",
    "\n",
    "        stripped_data = []\n",
    "        articles_processed_count = 0\n",
    "        for article in raw_data:\n",
    "            raw_html = article.get(\"html_content\", \"\")\n",
    "            source_url = article.get(\"source_url\", \"N/A\")\n",
    "            html_no_scripts = strip_unwanted_blocks(raw_html) # Apply function\n",
    "            stripped_entry = {\"source_url\": source_url, \"html_content\": html_no_scripts}\n",
    "            stripped_data.append(stripped_entry)\n",
    "            articles_processed_count += 1\n",
    "\n",
    "        with open(NO_SCRIPTS_OUTPUT_FILENAME, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(stripped_data, json_file, ensure_ascii=False, indent=4)\n",
    "        print(f\"\\n--- Script/Noscript/Comment Stripping Complete ---\")\n",
    "        print(f\"Successfully processed {articles_processed_count} articles.\")\n",
    "        print(f\"Saved results to {NO_SCRIPTS_OUTPUT_FILENAME}\")\n",
    "\n",
    "        # Display sample (still might have weird spacing from original source)\n",
    "        if stripped_data:\n",
    "             sample_output = stripped_data[0]['html_content']\n",
    "             first_meaningful_chunk = re.search(r'\\S.*\\S', sample_output, re.DOTALL)\n",
    "             if first_meaningful_chunk:\n",
    "                 print(\"\\n--- Sample Output after Stripping (First 500 chars of meaningful content) ---\")\n",
    "                 print(repr(first_meaningful_chunk.group(0)[:500]) + \"...\") # Use repr\n",
    "             else:\n",
    "                 print(\"\\n--- Sample Output after Stripping (First 500 chars) ---\")\n",
    "                 print(repr(sample_output[:500]) + \"...\") # Use repr\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during processing: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8e1b64",
   "metadata": {},
   "source": [
    "#### This cell performs the first stage of HTML cleaning. It reads the raw HTML data saved in `scraped_articles_final.json`. For each article's HTML content, it calls the `strip_unwanted_blocks` function. This function uses regular expressions (`re.sub`) to find and remove the contents of `<script>` tags, `<noscript>` tags, HTML comments (``), and inline JavaScript event handlers (like `onclick=\"...\"`), replacing these removed sections with empty strings. The resulting HTML, now stripped of most script-related content but retaining other tags, is saved for each article into a new file named `scraped_articles_NO_SCRIPTS.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf8595",
   "metadata": {},
   "source": [
    "## STEP 3 (Q6) - Remove HTML Tags & Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84177c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 60 articles from scraped_articles_NO_SCRIPTS.json\n",
      "\n",
      "Starting refined cleaning process on 60 articles...\n",
      "\n",
      "--- Cleaning Complete (Q6 - Tags/Styles/Entities/Special Chars Removed) ---\n",
      "Successfully processed (Q6 - v6) 60 articles.\n",
      "Intermediate cleaned data saved to scraped_articles_CLEANED.json\n",
      "NOTE: Run Cell 5 (Q8) next to normalize whitespace.\n",
      "\n",
      "--- Sample BEFORE Whitespace Normalization (First 500 characters) ---\n",
      "\"Son of hostage Amiram Cooper, whose body remains in Gaza, says 'it's not over'Skip to contentAdvertisementWatch LiveBritish Broadcasting CorporationSubscribeSign InHomeNewsSportBusinessInnovationCultureArtsTravelEarthAudioVideoLiveDocumentariesHomeNewsSportBusinessInnovationCultureArtsTravelEarthAudioVideoLiveDocumentariesWeatherNewslettersWatch LiveAdvertisement'It's not over,' says son of hostage whose body remains in Gaza7 days agoShareSaveAlice CuddySenior international reporter, in Tel Aviv\"...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "RAW_JSON_INPUT = 'scraped_articles_NO_SCRIPTS.json' # Input from Enhanced Cell 3\n",
    "CLEAN_JSON_OUTPUT = 'scraped_articles_CLEANED.json' # Output for Cell 5\n",
    "\n",
    "# --- Cleaning Regex Patterns ---\n",
    "# 1. Remove <style> blocks and content\n",
    "STYLE_TAG_PATTERN = r'<style\\b[^>]*>.*?</style>'\n",
    "# 2. (Optional but kept from previous) Remove common leftover JS patterns\n",
    "LEFTOVER_JS_PATTERN = r'(?:var|let|const|window\\.|document\\.)\\s*\\w+\\s*=[^;>]+[;>]|{\\s*\"?[^\"]+\"?\\s*:[^}]+};?'\n",
    "# 3. Remove HTML comments\n",
    "HTML_COMMENT_PATTERN = r''\n",
    "# 4. Remove common HTML entities like &nbsp; &amp; etc.\n",
    "HTML_ENTITY_PATTERN = r'&[a-zA-Z#0-9]+;'\n",
    "# 5. Remove all *other* remaining HTML tags\n",
    "HTML_TAG_PATTERN = r'<[^>]+>'\n",
    "# 6. *** Pattern to remove unwanted special characters (keeping basics + whitespace \\s) ***\n",
    "#    This pattern defines what to *keep*. The regex matches everything ELSE.\n",
    "SPECIAL_CHAR_PATTERN = r'[^\\w\\s\\.,!?\\'\\-]' # Keep word chars, whitespace, and basic punctuation ',.!?-'\n",
    "\n",
    "# --- Load the No-Script/No-Comment Data ---\n",
    "try:\n",
    "    with open(RAW_JSON_INPUT, 'r', encoding='utf-8') as f:\n",
    "        intermediate_data = json.load(f)\n",
    "    print(f\"Successfully loaded {len(intermediate_data)} articles from {RAW_JSON_INPUT}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File '{RAW_JSON_INPUT}' not found. Ensure Enhanced Cell 3 ran successfully.\")\n",
    "    intermediate_data = []\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load JSON data: {e}\")\n",
    "    intermediate_data = []\n",
    "\n",
    "cleaned_articles_data = []\n",
    "\n",
    "if intermediate_data:\n",
    "    print(f\"\\nStarting refined cleaning process on {len(intermediate_data)} articles...\")\n",
    "    articles_cleaned_count = 0\n",
    "    for article in intermediate_data:\n",
    "        text_to_clean = article.get(\"html_content\", \"\")\n",
    "        source_url = article.get(\"source_url\", \"N/A\")\n",
    "\n",
    "        # --- Apply Removals Sequentially ---\n",
    "        # 1. Remove Style Blocks (replace with empty string)\n",
    "        text_to_clean = re.sub(STYLE_TAG_PATTERN, '', text_to_clean, flags=re.DOTALL | re.IGNORECASE)\n",
    "        # 2. Remove Heuristic JS (replace with empty string)\n",
    "        text_to_clean = re.sub(LEFTOVER_JS_PATTERN, '', text_to_clean, flags=re.DOTALL | re.IGNORECASE)\n",
    "        # 3. Remove Comments (replace with empty string)\n",
    "        text_to_clean = re.sub(HTML_COMMENT_PATTERN, '', text_to_clean, flags=re.DOTALL)\n",
    "        # 4. Remove HTML Entities (replace with space)\n",
    "        text_to_clean = re.sub(HTML_ENTITY_PATTERN, ' ', text_to_clean)\n",
    "        # 5. Remove Remaining HTML Tags (replace with empty string)\n",
    "        text_to_clean = re.sub(HTML_TAG_PATTERN, '', text_to_clean)\n",
    "        # 6. *** Remove Special Characters (replace matches with space) ***\n",
    "        text_no_special_chars = re.sub(SPECIAL_CHAR_PATTERN, ' ', text_to_clean)\n",
    "\n",
    "        # Basic strip, but major whitespace cleanup happens in Cell 5\n",
    "        final_intermediate_text = text_no_special_chars.strip()\n",
    "\n",
    "        # --- Store Cleaned Data ---\n",
    "        cleaned_entry = {\n",
    "            \"source_url\": source_url,\n",
    "            \"clean_text\": final_intermediate_text # This text STILL has excess internal whitespace\n",
    "        }\n",
    "        cleaned_articles_data.append(cleaned_entry)\n",
    "        articles_cleaned_count +=1\n",
    "\n",
    "    # --- Save the Intermediate Cleaned Data ---\n",
    "    try:\n",
    "        with open(CLEAN_JSON_OUTPUT, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(cleaned_articles_data, json_file, ensure_ascii=False, indent=4)\n",
    "        print(f\"\\n--- Cleaning Complete (Q6 - Tags/Styles/Entities/Special Chars Removed) ---\")\n",
    "        print(f\"Successfully processed (Q6 - v6) {articles_cleaned_count} articles.\")\n",
    "        print(f\"Intermediate cleaned data saved to {CLEAN_JSON_OUTPUT}\")\n",
    "        print(\"NOTE: Run Cell 5 (Q8) next to normalize whitespace.\")\n",
    "\n",
    "        # --- Display Sample (will likely show excess whitespace) ---\n",
    "        if cleaned_articles_data:\n",
    "            sample_text = cleaned_articles_data[0]['clean_text'][:500]\n",
    "            print(\"\\n--- Sample BEFORE Whitespace Normalization (First 500 characters) ---\")\n",
    "            print(repr(sample_text) + \"...\") # Use repr()\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"ERROR: Could not save cleaned data to {CLEAN_JSON_OUTPUT}: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo data loaded from no-scripts file. Cannot proceed with cleaning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab50375",
   "metadata": {},
   "source": [
    "#### This cell executes the cleaning steps required by Question 6. It loads the partially cleaned HTML from `scraped_articles_NO_SCRIPTS.json`. It then applies several regular expression substitutions (`re.sub`) sequentially: removing `<style>` blocks, heuristic JavaScript remnants, HTML comments (though the pattern `r''` is non-functional here), HTML entities (replacing with spaces), and all remaining HTML tags (replacing with empty strings). Finally, it removes unwanted special characters (anything not alphanumeric, whitespace, or basic punctuation `.,!?'\\-`), replacing them with spaces. The resulting text, still containing excess whitespace, is saved to `scraped_articles_CLEANED.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8edd502",
   "metadata": {},
   "source": [
    "## STEP 4 (Q8) - Remove Unnecessary Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a182e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 60 articles from scraped_articles_CLEANED.json\n",
      "\n",
      "Starting whitespace normalization on 60 articles...\n",
      "\n",
      "--- Whitespace Normalization Complete ---\n",
      "Successfully normalized whitespace (Q8 v5) for 60 articles in 0.02 seconds.\n",
      "Final cleaned data saved back to scraped_articles_CLEANED.json\n",
      "\n",
      "--- Sample of FINAL Cleaned Text (First 500 characters) ---\n",
      "Son of hostage Amiram Cooper, whose body remains in Gaza, says 'it's not over'Skip to contentAdvertisementWatch LiveBritish Broadcasting CorporationSubscribeSign InHomeNewsSportBusinessInnovationCultureArtsTravelEarthAudioVideoLiveDocumentariesHomeNewsSportBusinessInnovationCultureArtsTravelEarthAudioVideoLiveDocumentariesWeatherNewslettersWatch LiveAdvertisement'It's not over,' says son of hostage whose body remains in Gaza7 days agoShareSaveAlice CuddySenior international reporter, in Tel Aviv...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "CLEAN_JSON_INPUT = 'scraped_articles_CLEANED.json' # Input from Q6 v6\n",
    "\n",
    "# --- Whitespace Normalization Pattern ---\n",
    "WHITESPACE_PATTERN = r'\\s+' # Matches one or more whitespace characters\n",
    "\n",
    "# --- Load the intermediate cleaned data ---\n",
    "try:\n",
    "    with open(CLEAN_JSON_INPUT, 'r', encoding='utf-8') as f:\n",
    "        partially_cleaned_data = json.load(f)\n",
    "    print(f\"Successfully loaded {len(partially_cleaned_data)} articles from {CLEAN_JSON_INPUT}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File '{CLEAN_JSON_INPUT}' not found. Please ensure the Q6 script ran successfully.\")\n",
    "    partially_cleaned_data = []\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load data: {e}\")\n",
    "    partially_cleaned_data = []\n",
    "\n",
    "if partially_cleaned_data:\n",
    "    print(f\"\\nStarting whitespace normalization on {len(partially_cleaned_data)} articles...\")\n",
    "    start_time = time.time()\n",
    "    articles_normalized_count = 0\n",
    "    # Iterate and update 'clean_text'\n",
    "    for article in partially_cleaned_data:\n",
    "        current_text = article.get(\"clean_text\", \"\")\n",
    "\n",
    "        # --- Normalize ALL whitespace ---\n",
    "        normalized_text = re.sub(WHITESPACE_PATTERN, ' ', current_text).strip()\n",
    "\n",
    "        # Overwrite the field\n",
    "        article[\"clean_text\"] = normalized_text\n",
    "        articles_normalized_count += 1\n",
    "\n",
    "    # --- Save the fully cleaned data back ---\n",
    "    try:\n",
    "        with open(CLEAN_JSON_INPUT, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(partially_cleaned_data, json_file, ensure_ascii=False, indent=4)\n",
    "        end_time = time.time()\n",
    "        print(f\"\\n--- Whitespace Normalization Complete ---\")\n",
    "        print(f\"Successfully normalized whitespace (Q8 v5) for {articles_normalized_count} articles in {end_time - start_time:.2f} seconds.\")\n",
    "        print(f\"Final cleaned data saved back to {CLEAN_JSON_INPUT}\")\n",
    "\n",
    "        # Show final sample\n",
    "        if partially_cleaned_data:\n",
    "            sample_text = partially_cleaned_data[0]['clean_text'][:500]\n",
    "            print(\"\\n--- Sample of FINAL Cleaned Text (First 500 characters) ---\")\n",
    "            print(sample_text + \"...\")\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"ERROR: Could not save final cleaned data to {CLEAN_JSON_INPUT}: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo intermediate cleaned data loaded. Cannot perform whitespace normalization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835eb7e5",
   "metadata": {},
   "source": [
    "#### This cell addresses Question 8 by normalizing whitespace. It reads the intermediate `scraped_articles_CLEANED.json` file produced by the previous step. For each article's text, it applies the regular expression `r'\\s+'` using `re.sub` to replace any sequence of one or more whitespace characters (spaces, tabs, newlines) with a single space (`' '`). It also uses the `.strip()` method to remove any leading or trailing whitespace. This fully cleaned and normalized text is then saved back, overwriting the `scraped_articles_CLEANED.json` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a613b5ae",
   "metadata": {},
   "source": [
    "## Q1 - Extracting URLs (from Raw HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea7cd476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting URL extraction from 60 articles...\n",
      "\n",
      "--- Extraction Complete ---\n",
      "Total articles processed: 60\n",
      "Total unique URLs extracted: 3666\n",
      "\n",
      "Sample Extracted URLs:\n",
      "1. https://ichef.bbci.co.uk/news/1536/cpsprodpb/23e2/live/24f0dc60-a845-11f0-92db-77261a15b9d2.png.webp\n",
      "2. https://ichef.bbci.co.uk/news/240/cpsprodpb/81d8/live/d175edc0-a842-11f0-92db-77261a15b9d2.png.webp\n",
      "3. https://www.bbc.com/news/articles/ckgywnjkrlqo\n",
      "4. https://media.cnn.com/api/v1/images/stellar/prod/045a27b3-fd48-4c69-bc4f-1409769c4825.jpg?q=h_562,w_1000,x_0,y_0/w_1280\n",
      "5. https://people.com/movies/jake-gyllenhaal-jamie-lee-curtis-living-together-covid-lockdown/\n",
      "6. https://bbc.com/news/articles/clykwd9e256o\n",
      "7. https://ichef.bbci.co.uk/news/800/cpsprodpb/64a5/live/89868750-a9c2-11f0-8da2-811fba9518ff.jpg.webp\n",
      "8. https://ichef.bbci.co.uk/news/480/cpsprodpb/e725/live/e73966a0-991f-11f0-928c-71dbb8619e94.jpg.webp\n",
      "9. https://ichef.bbci.co.uk/news/240/cpsprodpb/dda7/live/b22b30a0-a7cb-11f0-b50c-8f62428b85e3.jpg.webp\n",
      "10. https://media.cnn.com/api/v1/images/stellar/prod/230418143926-jake-gyllenhaal-jamie-lee-curtis-041723.jpg?c=original\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the name of the file created by your Selenium script\n",
    "JSON_INPUT_FILENAME = 'scraped_articles_final.json' \n",
    "\n",
    "# Load the scraped data from the JSON file\n",
    "try:\n",
    "    with open(JSON_INPUT_FILENAME, 'r', encoding='utf-8') as f:\n",
    "        scraped_articles_data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File '{JSON_INPUT_FILENAME}' not found. Please ensure the scraping script ran successfully.\")\n",
    "    scraped_articles_data = []\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load JSON data: {e}\")\n",
    "    scraped_articles_data = []\n",
    "\n",
    "# --- Regex Pattern for URLs ---\n",
    "# This pattern captures http or https, followed by any non-whitespace characters,\n",
    "# and is robust enough to catch most common web links.\n",
    "URL_PATTERN = r\"(https?:\\/\\/[^\\s\\\"]+)\" \n",
    "# NOTE: We use r\"...\" for raw string to handle backslashes correctly.\n",
    "\n",
    "all_extracted_urls = set() # Use a set to automatically store only unique URLs\n",
    "article_count = 0\n",
    "\n",
    "if scraped_articles_data:\n",
    "    print(f\"Starting URL extraction from {len(scraped_articles_data)} articles...\")\n",
    "    \n",
    "    for article in scraped_articles_data:\n",
    "        article_count += 1\n",
    "        html_content = article.get(\"html_content\", \"\")\n",
    "        \n",
    "        # Find all matches in the HTML content\n",
    "        found_urls = re.findall(URL_PATTERN, html_content)\n",
    "        \n",
    "        # Add the found URLs to the set\n",
    "        for url in found_urls:\n",
    "            all_extracted_urls.add(url)\n",
    "            \n",
    "    # --- Output Summary ---\n",
    "    print(\"\\n--- Extraction Complete ---\")\n",
    "    print(f\"Total articles processed: {article_count}\")\n",
    "    print(f\"Total unique URLs extracted: {len(all_extracted_urls)}\")\n",
    "    \n",
    "    # Display the first 10 extracted URLs for verification\n",
    "    print(\"\\nSample Extracted URLs:\")\n",
    "    for i, url in enumerate(list(all_extracted_urls)[:10]):\n",
    "        print(f\"{i+1}. {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101a7fb4",
   "metadata": {},
   "source": [
    "#### This cell answers Question 1 by extracting URLs. It specifically loads the *raw* HTML data collected by the scraper (`scraped_articles_final.json`). Using the regular expression `r\"(https?:\\/\\/[^\\s\\\"]+)\"`, it finds all substrings within the raw HTML that look like standard web links (starting with `http://` or `https://` and continuing until whitespace or a quote). `re.findall` is used to capture all such occurrences. These extracted URLs are stored in a Python set to automatically keep only unique ones, and a sample of the results is printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5759e88d",
   "metadata": {},
   "source": [
    "## Q2 - Extract Publication Dates (from Raw HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b21b7ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting date extraction from 60 articles...\n",
      "\n",
      "--- Extraction Complete ---\n",
      "Successfully extracted dates for 52 articles.\n",
      "Displaying a random sample of 10 extracted dates:\n",
      "------------------------------\n",
      "01. URL: https://www.bbc.com/news/articles/cj9zmeerp1xo?at_medium=RSS&at_campaign=rss | Date: October 1, 2025\n",
      "02. URL: https://www.aljazeera.com/news/2025/10/14/madagascar-president-dissolves-parliament-after-fleeing-army-backed-protest?traffic_source=rss | Date: 2025-10-14T12:38:43Z\n",
      "03. URL: https://www.aljazeera.com/video/newsfeed/2025/10/14/deadly-storm-batters-alaska-leaving-thousands-displaced?traffic_source=rss | Date: 2025-10-14T13:33:46Z\n",
      "04. URL: https://www.cnn.com/2023/04/17/entertainment/jamie-foxx-remains-hospitalized/index.html | Date: 2023-04-18T00:27:07Z\n",
      "05. URL: https://www.aljazeera.com/news/2025/10/14/mapping-the-rise-in-israeli-settler-attacks-across-the-occupied-west-bank?traffic_source=rss | Date: 2025-10-14T14:46:26Z\n",
      "06. URL: https://www.aljazeera.com/news/2025/10/14/who-is-in-charge-of-madagascar-after-president-rajoelina-flees?traffic_source=rss | Date: 2025-10-14T14:04:23Z\n",
      "07. URL: https://www.bbc.com/news/articles/cqjwlxjn1xgo?at_medium=RSS&at_campaign=rss | Date: November 19, 2024\n",
      "08. URL: https://www.aljazeera.com/sports/liveblog/2025/10/14/live-nigeria-vs-benin-caf-world-cup-qualifierfollo?traffic_source=rss | Date: 2025-10-14T12:33:01Z\n",
      "09. URL: https://www.cnn.com/business/live-news/fox-news-dominion-trial-04-18-23/index.html | Date: 2023-04-18T15:02:55Z\n",
      "10. URL: https://www.aljazeera.com/news/2025/10/14/russia-strikes-kharkiv-hospital-un-convoy-as-ukraine-seeks-us-tomahawks?traffic_source=rss | Date: 2025-10-14T15:32:31Z\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import random # <--- New import for random selection\n",
    "\n",
    "# --- Configuration ---\n",
    "JSON_INPUT_FILENAME = 'scraped_articles_final.json' \n",
    "extracted_dates_list = []\n",
    "SAMPLE_SIZE = 10 # <--- Define the size of the random sample\n",
    "\n",
    "# Load the scraped data \n",
    "try:\n",
    "    with open(JSON_INPUT_FILENAME, 'r', encoding='utf-8') as f:\n",
    "        scraped_articles_data = json.load(f)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ERROR: Could not load JSON data: {e}\")\n",
    "    scraped_articles_data = []\n",
    "\n",
    "# --- Comprehensive Date Regex Patterns ---\n",
    "# Run these patterns in order from most predictable to least predictable.\n",
    "DATE_PATTERNS = [\n",
    "    # 1. ISO 8601 format (e.g., 2025-10-14T11:51:31Z) - often in meta tags\n",
    "    r'(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z)',\n",
    "    # 2. YYYY/MM/DD format (e.g., 2025/10/14)\n",
    "    r'(\\d{4}/\\d{2}/\\d{2})',\n",
    "    # 3. Full Month DD, YYYY format (e.g., October 14, 2025)\n",
    "    r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}',\n",
    "    # 4. Abbreviated Month DD, YYYY format (e.g., Oct 14, 2025)\n",
    "    r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2},\\s+\\d{4}',\n",
    "]\n",
    "\n",
    "if scraped_articles_data:\n",
    "    print(f\"Starting date extraction from {len(scraped_articles_data)} articles...\")\n",
    "\n",
    "    # --- 1. Extraction Loop (Finds Dates for ALL Articles) ---\n",
    "    for article in scraped_articles_data:\n",
    "        html_content = article.get(\"html_content\", \"\")\n",
    "        extracted_date = \"NOT FOUND\"\n",
    "        \n",
    "        for pattern in DATE_PATTERNS:\n",
    "            # Use re.search for the first match, which is usually the publication date\n",
    "            match = re.search(pattern, html_content)\n",
    "            if match:\n",
    "                extracted_date = match.group(0) # Get the entire matched string\n",
    "                break # Stop searching once a match is found\n",
    "        \n",
    "        extracted_dates_list.append({\n",
    "            \"source_url\": article.get(\"source_url\", \"N/A\"),\n",
    "            \"publication_date\": extracted_date\n",
    "        })\n",
    "\n",
    "    # --- 2. Random Sampling (Select 10 Dates) ---\n",
    "    print(\"\\n--- Extraction Complete ---\")\n",
    "    \n",
    "    # Filter out 'NOT FOUND' entries before sampling for better verification\n",
    "    found_dates = [item for item in extracted_dates_list if item['publication_date'] != 'NOT FOUND']\n",
    "    \n",
    "    if len(found_dates) >= SAMPLE_SIZE:\n",
    "        # Use random.sample to get exactly SAMPLE_SIZE unique items\n",
    "        random_sample = random.sample(found_dates, SAMPLE_SIZE)\n",
    "        print(f\"Successfully extracted dates for {len(found_dates)} articles.\")\n",
    "        print(f\"Displaying a random sample of {SAMPLE_SIZE} extracted dates:\")\n",
    "    else:\n",
    "        # If fewer than 10 dates were found, use all of them\n",
    "        random_sample = found_dates\n",
    "        print(f\"âš ï¸ Only {len(found_dates)} dates were successfully extracted. Displaying all of them:\")\n",
    "\n",
    "\n",
    "    # --- Output Summary (Random Sample) ---\n",
    "    print(\"-\" * 30)\n",
    "    for i, item in enumerate(random_sample):\n",
    "        print(f\"{i+1:02d}. URL: {item['source_url']} | Date: {item['publication_date']}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea369fe",
   "metadata": {},
   "source": [
    "#### This cell addresses Question 2, extracting publication dates, again using the *raw* HTML data (`scraped_articles_final.json`). It defines a list of regex patterns (`DATE_PATTERNS`) matching common date formats (ISO 8601, YYYY/MM/DD, textual month formats). For each article, it iterates through these patterns, using `re.search` to find the *first* occurrence that matches any pattern. This first match is assumed to be the publication date and is stored. Finally, the code filters out articles where no date was found and displays a random sample of the extracted dates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00cccb6",
   "metadata": {},
   "source": [
    "## Q3 - Extract Author Names (from Cleaned Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4de71286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting author extraction from 60 CLEANED articles...\n",
      "\n",
      "--- Extraction Complete ---\n",
      "Successfully extracted author names for 34 articles.\n",
      "Displaying a random sample of 10 extracted authors:\n",
      "------------------------------\n",
      "01. URL: https://www.cnn.com/2023/04/18/media/fox-dominion-settlement/index.html | Author: Marshall Cohen\n",
      "02. URL: https://www.cnn.com/2023/04/18/politics/white-house-toddler/index.html | Author: Arlette Saenz\n",
      "03. URL: https://www.aljazeera.com/news/2025/10/14/who-is-in-charge-of-madagascar-after-president-rajoelina-flees?traffic_source=rss | Author: Shola Lawal\n",
      "04. URL: https://www.cnn.com/business/live-news/fox-news-dominion-trial-04-18-23/h_8d51e3ae2714edaa0dace837305d03b8 | Author: Catherine Thorbecke\n",
      "05. URL: https://www.cnn.com/2023/04/18/entertainment/jake-gyllenhaal-jamie-lee-curtis-pandemic-living/index.html | Author: Marianne Garvey\n",
      "06. URL: https://www.bbc.com/news/articles/c62e3pny6p7o?at_medium=RSS&at_campaign=rss | Author: Prime Minister Bart\n",
      "07. URL: https://www.cnn.com/2023/04/18/media/netflix-dvd-red-envelopes/index.html | Author: Allison Morrow\n",
      "08. URL: https://www.cnn.com/2023/04/18/us/benadryl-tiktok-challenge-teen-death-wellness/index.html | Author: Michelle Watson\n",
      "09. URL: https://www.aljazeera.com/news/2025/10/14/russia-strikes-kharkiv-hospital-un-convoy-as-ukraine-seeks-us-tomahawks?traffic_source=rss | Author: Tim Hume\n",
      "10. URL: https://www.cnn.com/2023/04/17/health/rise-type-2-diabetes-global-wellness/index.html | Author: Sandee La\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "CLEAN_JSON_INPUT = 'scraped_articles_CLEANED.json' \n",
    "AUTHOR_SAMPLE_SIZE = 10 \n",
    "\n",
    "# Load the CLEANED data \n",
    "try:\n",
    "    with open(CLEAN_JSON_INPUT, 'r', encoding='utf-8') as f:\n",
    "        cleaned_data = json.load(f)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ERROR: Could not load cleaned JSON data: {e}\")\n",
    "    cleaned_data = []\n",
    "\n",
    "# --- Author Regex Pattern ---\n",
    "# Matches common \"By [Name]\" phrases, expecting 2-6 capitalized words.\n",
    "AUTHOR_PATTERN = r'(?:By|BY|by|Author|AUTHORED BY|REPORTER|Credit)\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+){1,5})'\n",
    "\n",
    "extracted_authors_list = []\n",
    "article_count = 0\n",
    "\n",
    "if cleaned_data:\n",
    "    print(f\"Starting author extraction from {len(cleaned_data)} CLEANED articles...\")\n",
    "\n",
    "    for article in cleaned_data:\n",
    "        article_count += 1\n",
    "        clean_text = article.get(\"clean_text\", \"\")\n",
    "        extracted_author = \"NOT FOUND\"\n",
    "        \n",
    "        # Search the entire text for the author pattern\n",
    "        match = re.search(AUTHOR_PATTERN, clean_text)\n",
    "        \n",
    "        if match:\n",
    "            # Group 1 holds the captured name\n",
    "            extracted_author = match.group(1).strip()\n",
    "        \n",
    "        extracted_authors_list.append({\n",
    "            \"source_url\": article.get(\"source_url\", \"N/A\"),\n",
    "            \"author_name\": extracted_author\n",
    "        })\n",
    "\n",
    "    # --- Random Sampling (Select 10 Authors) ---\n",
    "    print(\"\\n--- Extraction Complete ---\")\n",
    "    \n",
    "    found_authors = [item for item in extracted_authors_list if item['author_name'] != 'NOT FOUND']\n",
    "    \n",
    "    if len(found_authors) >= AUTHOR_SAMPLE_SIZE:\n",
    "        random_sample = random.sample(found_authors, AUTHOR_SAMPLE_SIZE)\n",
    "        print(f\"Successfully extracted author names for {len(found_authors)} articles.\")\n",
    "        print(f\"Displaying a random sample of {AUTHOR_SAMPLE_SIZE} extracted authors:\")\n",
    "    else:\n",
    "        random_sample = found_authors\n",
    "        print(f\"Only {len(found_authors)} author names were successfully extracted. Displaying all of them:\")\n",
    "\n",
    "\n",
    "    # --- Output Summary (Random Sample) ---\n",
    "    print(\"-\" * 30)\n",
    "    for i, item in enumerate(random_sample):\n",
    "        print(f\"{i+1:02d}. URL: {item['source_url']} | Author: {item['author_name']}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f38af1",
   "metadata": {},
   "source": [
    "#### This cell answers Question 3, extracting author names from the *fully cleaned* text (`scraped_articles_CLEANED.json`). It uses `re.search` with the pattern `r'(?:By|...|Credit)\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+){1,5})'`. This pattern looks for common attribution keywords (like \"By\", \"Author\") followed by whitespace, and then captures (in Group 1) a sequence of 2 to 6 capitalized words assumed to be the author's name. If a match is found, the captured name is extracted. A random sample of the found author names is then printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e734ae",
   "metadata": {},
   "source": [
    "## Q4 - Extract Email Addresses (from Cleaned Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "714c0601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting email extraction from 60 CLEANED articles...\n",
      "\n",
      "--- Extraction Complete ---\n",
      "Only 0 email addresses were extracted. Displaying all of them:\n",
      "------------------------------\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "# File containing the cleaned text content\n",
    "CLEAN_JSON_INPUT = 'scraped_articles_CLEANED.json' \n",
    "EMAIL_SAMPLE_SIZE = 10 \n",
    "\n",
    "# Load the cleaned data \n",
    "try:\n",
    "    with open(CLEAN_JSON_INPUT, 'r', encoding='utf-8') as f:\n",
    "        cleaned_data = json.load(f)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ERROR: Could not load cleaned JSON data: {e}\")\n",
    "    cleaned_data = []\n",
    "\n",
    "# --- Email Regex Pattern ---\n",
    "# Matches standard email structure: username@domain.tld\n",
    "EMAIL_PATTERN = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}'\n",
    "\n",
    "all_extracted_emails = set()\n",
    "article_count = 0\n",
    "\n",
    "if cleaned_data:\n",
    "    print(f\"Starting email extraction from {len(cleaned_data)} CLEANED articles...\")\n",
    "\n",
    "    for article in cleaned_data:\n",
    "        article_count += 1\n",
    "        # Use the 'clean_text' field\n",
    "        clean_text = article.get(\"clean_text\", \"\")\n",
    "        \n",
    "        # Find all email matches in the cleaned text\n",
    "        found_emails = re.findall(EMAIL_PATTERN, clean_text)\n",
    "        \n",
    "        # Add the found emails to a set to ensure uniqueness\n",
    "        for email in found_emails:\n",
    "            # Simple filter to exclude common non-author/spam-like emails\n",
    "            if not email.startswith(('support@', 'info@', 'admin@', 'careers@')):\n",
    "                all_extracted_emails.add(email)\n",
    "            \n",
    "    # --- 2. Random Sampling (Select 10 Emails) ---\n",
    "    print(\"\\n--- Extraction Complete ---\")\n",
    "    \n",
    "    if len(all_extracted_emails) >= EMAIL_SAMPLE_SIZE:\n",
    "        # Convert set to list for random sampling\n",
    "        random_sample = random.sample(list(all_extracted_emails), EMAIL_SAMPLE_SIZE)\n",
    "        print(f\"Successfully extracted {len(all_extracted_emails)} unique email addresses.\")\n",
    "        print(f\"Displaying a random sample of {EMAIL_SAMPLE_SIZE} extracted emails:\")\n",
    "    else:\n",
    "        random_sample = list(all_extracted_emails)\n",
    "        print(f\"Only {len(all_extracted_emails)} email addresses were extracted. Displaying all of them:\")\n",
    "\n",
    "\n",
    "    # --- Output Summary (Random Sample) ---\n",
    "    print(\"-\" * 30)\n",
    "    for i, email in enumerate(random_sample):\n",
    "        print(f\"{i+1:02d}. Email: {email}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# The email list is stored in 'all_extracted_emails' if you need it for later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661405f5",
   "metadata": {},
   "source": [
    "#### This cell addresses Question 4 by extracting email addresses from the *cleaned* text (`scraped_articles_CLEANED.json`). It uses `re.findall` and a standard email regex pattern (`r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}'`) to find all strings matching the typical email structure. The results are then filtered to exclude common service addresses (like \"info@\", \"support@\"). All unique, potentially relevant email addresses are collected in a set, and a random sample (or all, if few are found) is displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9139147e",
   "metadata": {},
   "source": [
    "## Q5 - Extract Phone Numbers (from Cleaned Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc9a6544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 60 articles from scraped_articles_CLEANED.json\n",
      "\n",
      "--- Testing Regex Pattern ---\n",
      "String: 'Call 555-123-4567 for info.' -> Found Tuples: [('', '555-', '123-', '4567')] -> Reconstructed & Validated: ['555-123-4567']\n",
      "String: 'Contact us at (123) 456-7890.' -> Found Tuples: [('', '(123) ', '456-', '7890')] -> Reconstructed & Validated: ['(123) 456-7890']\n",
      "String: 'Number: +1 987 654 3210.' -> Found Tuples: [('+1 ', '987 ', '654 ', '3210')] -> Reconstructed & Validated: ['+1 987 654 3210']\n",
      "String: 'Maybe 456-7890 or just 1234567.' -> Found Tuples: [] -> Reconstructed & Validated: []\n",
      "String: 'Not a phone 1234.' -> Found Tuples: [] -> Reconstructed & Validated: []\n",
      "String: 'Text with 2024 year.' -> Found Tuples: [] -> Reconstructed & Validated: []\n",
      "------------------------------\n",
      "\n",
      "Starting phone number extraction from 60 CLEANED articles...\n",
      "\n",
      "Extraction loop finished in 0.02 seconds.\n",
      "Total raw regex matches (tuples found): 0\n",
      "\n",
      "--- Extraction Complete ---\n",
      "No validated phone numbers were extracted.\n",
      "Possible reasons: No numbers in text, regex too specific, or cleaning issues.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "import time # Ensure time is imported\n",
    "\n",
    "# --- Configuration ---\n",
    "CLEAN_JSON_INPUT = 'scraped_articles_CLEANED.json'\n",
    "NUMBER_SAMPLE_SIZE = 10\n",
    "\n",
    "# --- Load the cleaned data ---\n",
    "try:\n",
    "    with open(CLEAN_JSON_INPUT, 'r', encoding='utf-8') as f:\n",
    "        cleaned_data = json.load(f)\n",
    "    print(f\"Successfully loaded {len(cleaned_data)} articles from {CLEAN_JSON_INPUT}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load cleaned JSON data: {e}\")\n",
    "    cleaned_data = []\n",
    "\n",
    "# --- Phone Number Regex Pattern ---\n",
    "# Captures standard global and North American formats.\n",
    "PHONE_NUMBER_PATTERN = r'(\\+\\d{1,3}[-.\\s]?)?(\\(?\\d{3}\\)?[-.\\s]?)(\\d{3}[-.\\s]?)(\\d{4})'\n",
    "\n",
    "# --- Test the Regex ---\n",
    "print(\"\\n--- Testing Regex Pattern ---\")\n",
    "test_strings = [\n",
    "    \"Call 555-123-4567 for info.\",\n",
    "    \"Contact us at (123) 456-7890.\",\n",
    "    \"Number: +1 987 654 3210.\",\n",
    "    \"Maybe 456-7890 or just 1234567.\", # 7 digits straight\n",
    "    \"Not a phone 1234.\",\n",
    "    \"Text with 2024 year.\"\n",
    "]\n",
    "for test_str in test_strings:\n",
    "    matches = re.findall(PHONE_NUMBER_PATTERN, test_str)\n",
    "    reconstructed = [\"\".join(m).strip() for m in matches if len(re.sub(r'[^\\d]', '', \"\".join(m))) >= 7]\n",
    "    print(f\"String: '{test_str}' -> Found Tuples: {matches} -> Reconstructed & Validated: {reconstructed}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- Extraction from Articles ---\n",
    "all_extracted_numbers = set()\n",
    "article_count = 0\n",
    "found_tuples_count = 0 # Track raw matches\n",
    "\n",
    "if cleaned_data:\n",
    "    print(f\"\\nStarting phone number extraction from {len(cleaned_data)} CLEANED articles...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for article_index, article in enumerate(cleaned_data): # Get index for reporting\n",
    "        article_count += 1\n",
    "        clean_text = article.get(\"clean_text\", \"\")\n",
    "        if not clean_text: continue # Skip empty articles\n",
    "\n",
    "        # Find all potential matches (returns list of tuples)\n",
    "        found_tuples = re.findall(PHONE_NUMBER_PATTERN, clean_text)\n",
    "\n",
    "        if found_tuples: # If the regex found *anything*\n",
    "            found_tuples_count += len(found_tuples)\n",
    "            # Optional: Print raw tuples found for debugging\n",
    "            # print(f\"  Raw tuples found in article {article_index}: {found_tuples}\")\n",
    "\n",
    "            # Process the results\n",
    "            for match_tuple in found_tuples:\n",
    "                number_str = \"\".join(match_tuple).strip()\n",
    "                digits_only = re.sub(r'[^\\d]', '', number_str)\n",
    "\n",
    "                # Validation: at least 7 digits\n",
    "                if len(digits_only) >= 7:\n",
    "                    all_extracted_numbers.add(number_str)\n",
    "                    # Optional: Print validated number found\n",
    "                    # print(f\"  Validated number in article {article_index}: {number_str}\")\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nExtraction loop finished in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Total raw regex matches (tuples found): {found_tuples_count}\")\n",
    "\n",
    "    # --- Random Sampling (Select 10 Phone Numbers) ---\n",
    "    print(\"\\n--- Extraction Complete ---\")\n",
    "\n",
    "    if len(all_extracted_numbers) >= NUMBER_SAMPLE_SIZE:\n",
    "        random_sample = random.sample(list(all_extracted_numbers), NUMBER_SAMPLE_SIZE)\n",
    "        print(f\"Successfully extracted {len(all_extracted_numbers)} unique validated phone numbers.\")\n",
    "        print(f\"Displaying a random sample of {NUMBER_SAMPLE_SIZE} extracted phone numbers:\")\n",
    "    elif len(all_extracted_numbers) > 0:\n",
    "         random_sample = list(all_extracted_numbers)\n",
    "         print(f\"Successfully extracted {len(all_extracted_numbers)} unique validated phone numbers.\")\n",
    "         print(f\"Displaying all extracted phone numbers:\")\n",
    "    else: # No numbers found\n",
    "        random_sample = []\n",
    "        print(\"No validated phone numbers were extracted.\")\n",
    "        print(\"Possible reasons: No numbers in text, regex too specific, or cleaning issues.\")\n",
    "\n",
    "\n",
    "    # --- Output Summary (Random Sample) ---\n",
    "    if random_sample:\n",
    "        print(\"-\" * 30)\n",
    "        for i, number in enumerate(random_sample):\n",
    "            print(f\"{i+1:02d}. Phone: {number}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo cleaned data loaded. Cannot extract phone numbers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427bcce4",
   "metadata": {},
   "source": [
    "#### This cell answers Question 5 by attempting to extract phone numbers from the *cleaned* text (`scraped_articles_CLEANED.json`). It includes a section to test its regex pattern, `r'(\\+\\d{1,3}[-.\\s]?)?(\\(?\\d{3}\\)?[-.\\s]?)(\\d{3}[-..\\s]?)(\\d{4})'`, against sample strings. The pattern uses optional groups to match various common US/international formats. The code uses `re.findall` on each article's text, reconstructs the number from the matched groups, and validates that the result contains at least 7 digits before adding it to a set. Finally, it reports that zero validated numbers were found in the dataset using this specific pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd201d",
   "metadata": {},
   "source": [
    "## Q7 - Extract Organizations/Companies (from Cleaned Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d0b8392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting organization extraction from 60 CLEANED articles...\n",
      "\n",
      "--- Extraction Complete ---\n",
      "Successfully extracted 26 unique organizations/companies.\n",
      "Displaying a random sample of 10 extracted names:\n",
      "------------------------------\n",
      "01. Organization: Tufts University\n",
      "02. Organization: Asahi Group\n",
      "03. Organization: Birmingham City Council\n",
      "04. Organization: Every West Bank\n",
      "05. Organization: Beef Products Inc\n",
      "06. Organization: Arturo Jimenez Anadolu Agency\n",
      "07. Organization: Nasser Hospital\n",
      "08. Organization: News Corp\n",
      "09. Organization: World Bank\n",
      "10. Organization: Ahli Arab Hospital\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "# File containing the cleaned text content\n",
    "CLEAN_JSON_INPUT = 'scraped_articles_CLEANED.json' \n",
    "ORG_SAMPLE_SIZE = 10 \n",
    "\n",
    "# Load the cleaned data \n",
    "try:\n",
    "    with open(CLEAN_JSON_INPUT, 'r', encoding='utf-8') as f:\n",
    "        cleaned_data = json.load(f)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load cleaned JSON data: {e}\")\n",
    "    cleaned_data = []\n",
    "\n",
    "# --- Organization Regex Pattern (Suffix-Based) ---\n",
    "# Targets capitalized phrases ending in a formal business/organization suffix.\n",
    "ORG_PATTERN = r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\s+(?:Corp|Inc|Ltd|GmbH|Co|Foundation|Agency|Council|Group|Bank|Hospital|University)\\b)'\n",
    "\n",
    "all_extracted_orgs = set()\n",
    "article_count = 0\n",
    "\n",
    "if cleaned_data:\n",
    "    print(f\"Starting organization extraction from {len(cleaned_data)} CLEANED articles...\")\n",
    "\n",
    "    for article in cleaned_data:\n",
    "        article_count += 1\n",
    "        clean_text = article.get(\"clean_text\", \"\")\n",
    "        \n",
    "        # Find all organizational matches in the clean text\n",
    "        # re.MULTILINE is not needed here, but re.IGNORECASE would make it too broad.\n",
    "        found_orgs = re.findall(ORG_PATTERN, clean_text)\n",
    "        \n",
    "        # Add the found organizations to a set for uniqueness\n",
    "        for org in found_orgs:\n",
    "            # We enforce a minimum length and filter out common single words to reduce noise\n",
    "            if len(org.split()) >= 2 and org.lower() not in ['group', 'bank', 'company']:\n",
    "                all_extracted_orgs.add(org.strip())\n",
    "            \n",
    "    # --- 2. Random Sampling (Select 10 Organizations) ---\n",
    "    print(\"\\n--- Extraction Complete ---\")\n",
    "    \n",
    "    if len(all_extracted_orgs) >= ORG_SAMPLE_SIZE:\n",
    "        random_sample = random.sample(list(all_extracted_orgs), ORG_SAMPLE_SIZE)\n",
    "        print(f\"Successfully extracted {len(all_extracted_orgs)} unique organizations/companies.\")\n",
    "        print(f\"Displaying a random sample of {ORG_SAMPLE_SIZE} extracted names:\")\n",
    "    else:\n",
    "        random_sample = list(all_extracted_orgs)\n",
    "        print(f\"Only {len(all_extracted_orgs)} names were extracted. Displaying all of them:\")\n",
    "\n",
    "\n",
    "    # --- Output Summary (Random Sample) ---\n",
    "    print(\"-\" * 30)\n",
    "    for i, org in enumerate(random_sample):\n",
    "        print(f\"{i+1:02d}. Organization: {org}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ff8b2e",
   "metadata": {},
   "source": [
    "#### This cell addresses Question 7 by extracting potential organization names from the *cleaned* text (`scraped_articles_CLEANED.json`). It uses `re.findall` with the pattern `r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\s+(?:Corp|...|University)\\b)'`. This pattern searches for sequences of capitalized words followed by a common organizational suffix (Inc, Ltd, Agency, University, etc.) ending at a word boundary. The results are filtered to include only multi-word names and exclude generic matches like \"Bank\". Unique organization names found are collected, and a random sample is displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bba966",
   "metadata": {},
   "source": [
    "## Q9 - Extracting Titles/Headlines (from No-Script HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3d56edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting headline extraction from 60 RAW articles (using <title> tag)...\n",
      "\n",
      "--- Extraction Complete ---\n",
      "Successfully extracted headlines for 60 articles.\n",
      "Displaying a random sample of 10 extracted headlines:\n",
      "------------------------------\n",
      "1. URL: https://www.bbc.com/news/articles/clykwd9e256o?at_medium=RSS&at_campaign=rss | Headline: Cuban dissident JosÃ© Daniel Ferrer arrives in US exile\n",
      "2. URL: https://www.cnn.com/2023/04/17/media/dominion-fox-news-allegations/index.html | Headline: Here are the Fox broadcasts and tweets Dominion says were defamatory\n",
      "3. URL: https://www.cnn.com/2023/04/17/entertainment/jamie-foxx-remains-hospitalized/index.html | Headline: Jamie Foxx remains hospitalized nearly a week after experiencing â€˜medical complicationâ€™\n",
      "4. URL: https://www.cnn.com/2023/04/18/politics/mccarthy-biden-debt-ceiling/index.html | Headline: The US economy could depend on McCarthy corralling his extremist Republican troops\n",
      "5. URL: https://www.bbc.com/news/articles/c0rpwk51qxro?at_medium=RSS&at_campaign=rss | Headline: Asahi ransomware attack: Personal data potentially stolen\n",
      "6. URL: https://www.aljazeera.com/news/2025/10/14/mapping-the-rise-in-israeli-settler-attacks-across-the-occupied-west-bank?traffic_source=rss | Headline: play\n",
      "7. URL: https://www.bbc.com/news/articles/c4g92xq8wdlo?at_medium=RSS&at_campaign=rss | Headline: Paraguay â€“ the Silicon Valley of South America?\n",
      "8. URL: https://www.aljazeera.com/news/2025/10/14/torrential-rains-collapse-venezuelan-gold-mine-killing-14?traffic_source=rss | Headline: play\n",
      "9. URL: https://www.cnn.com/europe/live-news/russia-ukraine-war-news-04-18-23/index.html | Headline: April 18, 2023\n",
      "10. URL: https://www.cnn.com/videos/us/2023/04/18/jeremy-renner-snowplow-accident-bodycam-nc-melas-contd-vpx.cnn | Headline: Newly released video shows scene of Jeremy Rennerâ€™s snowplow accident\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "# *** IMPORTANT: Switching back to the RAW HTML input file ***\n",
    "JSON_INPUT_FILENAME = 'scraped_articles_NO_SCRIPTS.json' \n",
    "HEADLINE_SAMPLE_SIZE = 10 \n",
    "\n",
    "# Load the RAW scraped data \n",
    "try:\n",
    "    with open(JSON_INPUT_FILENAME, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load RAW JSON data: {e}\")\n",
    "    raw_data = []\n",
    "\n",
    "# --- Headline Regex Pattern (Targeting <title> tag) ---\n",
    "# This is a highly accurate method for extracting the headline from raw HTML.\n",
    "HEADLINE_PATTERN = r'<title>(.*?)</title>'\n",
    "\n",
    "extracted_headlines_list = []\n",
    "article_count = 0\n",
    "\n",
    "if raw_data:\n",
    "    print(f\"Starting headline extraction from {len(raw_data)} RAW articles (using <title> tag)...\")\n",
    "\n",
    "    for article in raw_data:\n",
    "        article_count += 1\n",
    "        # Use the 'html_content' field from the raw data\n",
    "        raw_html = article.get(\"html_content\", \"\")\n",
    "        extracted_headline = \"NOT FOUND\"\n",
    "        \n",
    "        # Use re.search because the title tag only appears once per document\n",
    "        match = re.search(HEADLINE_PATTERN, raw_html, re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        if match:\n",
    "            # Group 1 holds the captured title text\n",
    "            # We strip whitespace and clean up any potential leftover entities\n",
    "            headline = match.group(1).strip()\n",
    "            # Clean up common elements like ' | CNN' or ' - BBC News'\n",
    "            headline = re.sub(r' \\| .*$', '', headline) \n",
    "            headline = re.sub(r' - .*$', '', headline)\n",
    "            extracted_headline = headline\n",
    "        \n",
    "        extracted_headlines_list.append({\n",
    "            \"source_url\": article.get(\"source_url\", \"N/A\"),\n",
    "            \"headline\": extracted_headline\n",
    "        })\n",
    "\n",
    "    # --- 2. Random Sampling (Select 10 Headlines) ---\n",
    "    print(\"\\n--- Extraction Complete ---\")\n",
    "    \n",
    "    found_headlines = [item for item in extracted_headlines_list if item['headline'] != 'NOT FOUND']\n",
    "    \n",
    "    if len(found_headlines) >= HEADLINE_SAMPLE_SIZE:\n",
    "        random_sample = random.sample(found_headlines, HEADLINE_SAMPLE_SIZE)\n",
    "        print(f\"Successfully extracted headlines for {len(found_headlines)} articles.\")\n",
    "        print(f\"Displaying a random sample of {HEADLINE_SAMPLE_SIZE} extracted headlines:\")\n",
    "    else:\n",
    "        random_sample = found_headlines\n",
    "        print(f\"Only {len(found_headlines)} headlines were extracted. Displaying all of them:\")\n",
    "\n",
    "\n",
    "    # --- Output Summary (Random Sample) ---\n",
    "    print(\"-\" * 30)\n",
    "    for i, item in enumerate(random_sample):\n",
    "        # Limit headline output for clean display\n",
    "        print(f\"{i+1:01d}. URL: {item['source_url']} | Headline: {item['headline']}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce069b8",
   "metadata": {},
   "source": [
    "#### This cell answers Question 9 by extracting article headlines. It reads the partially cleaned HTML from `scraped_articles_NO_SCRIPTS.json` (output of Cell 8). It employs `re.search` with the pattern `r'<title>(.*?)</title>'` to capture the text content between the HTML title tags, ignoring case and handling potential newlines. Simple post-processing using `re.sub` is applied to remove common site name suffixes often appended to titles (e.g., \" | CNN\"). The extracted headlines are collected, and a random sample is printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc45377e",
   "metadata": {},
   "source": [
    "## Q10 - Extracting Important Events/Incidents (from Cleaned Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b96d686f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 60 articles from scraped_articles_CLEANED.json\n",
      "\n",
      "Starting event extraction from 60 articles...\n",
      "\n",
      "Processed 60 articles.\n",
      "\n",
      "--- Extraction Complete ---\n",
      "Successfully extracted 100 unique potential event mentions.\n",
      "Displaying a random sample of 10 extracted events:\n",
      "------------------------------\n",
      "1. Event: Conference on Arms Control\n",
      "2. Event: cancelled ceremonies\n",
      "3. Event: Hamas attacked on\n",
      "4. Event: war in Ukraine and called for peace in the conflict\n",
      "5. Event: Commerce announced export controls preventing\n",
      "6. Event: protest in front of Argentina\n",
      "7. Event: war in Gaza began\n",
      "8. Event: war on the Palestinian population after the ceasefire\n",
      "9. Event: strike on alleged drug vessel near VenezuelaNobel PrizeVenezuelaRelatedChinese Nobel laureate and ph...\n",
      "10. Event: declared in Proclamation\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "CLEAN_JSON_INPUT = 'scraped_articles_CLEANED.json' # Input: Fully cleaned text\n",
    "EVENT_SAMPLE_SIZE = 10\n",
    "\n",
    "# --- NEW Event Extraction Function ---\n",
    "def extract_events(text):\n",
    "    \"\"\"Extracts potential events using multiple specific regex patterns.\"\"\"\n",
    "    events = []\n",
    "    if text is None: # Handle None input\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # Pattern 1: Events with action verbs (attacked, killed, arrested, etc.)\n",
    "        events.extend(re.findall(r'[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\s+(?:attacked|killed|arrested|bombed|invaded|struck|destroyed|launched|conducted|announced|declared|signed|passed|voted|elected|resigned|died|crashed|exploded|collapsed)\\s+[a-z\\s]+', text))\n",
    "\n",
    "        # Pattern 2: Natural disasters and catastrophes\n",
    "        events.extend(re.findall(r'(?:earthquake|tsunami|hurricane|tornado|flood|wildfire|storm|cyclone|typhoon|avalanche|landslide|drought|famine)\\s+(?:in|hit|struck|devastated|destroyed)\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*', text, re.IGNORECASE))\n",
    "\n",
    "        # Pattern 3: Political/military events\n",
    "        events.extend(re.findall(r'(?:summit|conference|protest|rally|demonstration|war|conflict|battle|attack|strike|raid|invasion|election|referendum|coup|revolution)\\s+(?:in|at|on)\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*', text, re.IGNORECASE))\n",
    "\n",
    "        # Pattern 4: Incidents with casualties\n",
    "        events.extend(re.findall(r'\\d+\\s+(?:people|civilians|soldiers|protesters|victims)\\s+(?:killed|injured|wounded|died|missing|evacuated|arrested|detained)', text, re.IGNORECASE))\n",
    "\n",
    "        # Pattern 5: Major announcements\n",
    "        events.extend(re.findall(r'(?:announced|declared|imposed|launched|initiated|suspended|cancelled|postponed)\\s+(?:a|an|the)?\\s*[a-z\\s]{3,30}', text, re.IGNORECASE))\n",
    "\n",
    "        # Pattern 6: Specific event names (capitalized multi-word phrases ending in keywords)\n",
    "        events.extend(re.findall(r'\\b(?:[A-Z][a-z]+\\s+){2,4}(?:Summit|Conference|Agreement|Treaty|Accord|Crisis|Disaster|Attack|Incident)\\b', text))\n",
    "\n",
    "        # --- Filtering within the function ---\n",
    "        # 1. Clean whitespace\n",
    "        cleaned_events = [e.strip() for e in events]\n",
    "        # 2. Filter by length\n",
    "        cleaned_events = [e for e in cleaned_events if 10 < len(e) < 150]\n",
    "        # 3. Filter common sentence starters\n",
    "        cleaned_events = [e for e in cleaned_events if not e.lower().startswith(('the ', 'this ', 'that '))]\n",
    "        # 4. Remove duplicates\n",
    "        unique_events = list(set(cleaned_events))\n",
    "\n",
    "        return unique_events\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Regex error during event extraction - {e}\")\n",
    "        return [] # Return empty list on error for this article\n",
    "\n",
    "\n",
    "# --- Load the Cleaned Data ---\n",
    "try:\n",
    "    with open(CLEAN_JSON_INPUT, 'r', encoding='utf-8') as f:\n",
    "        cleaned_data = json.load(f)\n",
    "    print(f\"Successfully loaded {len(cleaned_data)} articles from {CLEAN_JSON_INPUT}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load cleaned JSON data: {e}\")\n",
    "    cleaned_data = []\n",
    "\n",
    "# --- Main Extraction Loop ---\n",
    "all_extracted_events = set() # Use a set to store unique events across all articles\n",
    "\n",
    "if cleaned_data:\n",
    "    print(f\"\\nStarting event extraction from {len(cleaned_data)} articles...\")\n",
    "    articles_processed_count = 0\n",
    "    for article in cleaned_data:\n",
    "        clean_text = article.get(\"clean_text\", \"\")\n",
    "\n",
    "        # Call the new function to get events for this article\n",
    "        found_events = extract_events(clean_text)\n",
    "\n",
    "        # Add the unique events found in this article to the overall set\n",
    "        if found_events:\n",
    "            all_extracted_events.update(found_events)\n",
    "        articles_processed_count += 1\n",
    "\n",
    "    print(f\"\\nProcessed {articles_processed_count} articles.\")\n",
    "\n",
    "    # --- Random Sampling and Output ---\n",
    "    print(\"\\n--- Extraction Complete ---\")\n",
    "\n",
    "    if not all_extracted_events:\n",
    "        print(\"No events extracted matching the defined patterns.\")\n",
    "    elif len(all_extracted_events) >= EVENT_SAMPLE_SIZE:\n",
    "        random_sample = random.sample(list(all_extracted_events), EVENT_SAMPLE_SIZE)\n",
    "        print(f\"Successfully extracted {len(all_extracted_events)} unique potential event mentions.\")\n",
    "        print(f\"Displaying a random sample of {EVENT_SAMPLE_SIZE} extracted events:\")\n",
    "    else: # Fewer than SAMPLE_SIZE found\n",
    "        random_sample = list(all_extracted_events)\n",
    "        print(f\"Only {len(all_extracted_events)} unique potential event mentions were extracted. Displaying all of them:\")\n",
    "\n",
    "    # --- Output Summary (Random Sample) ---\n",
    "    if random_sample:\n",
    "        print(\"-\" * 30)\n",
    "        for i, event in enumerate(random_sample):\n",
    "            # Limit length for display if needed\n",
    "            print(f\"{i+1:01d}. Event: {event[:100]}{'...' if len(event)>100 else ''}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo cleaned data loaded. Cannot extract events.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6459cf",
   "metadata": {},
   "source": [
    "#### This cell answers Question 10 by extracting potential event mentions from the *cleaned* text (`scraped_articles_CLEANED.json`). It defines and uses a function, `extract_events`, which applies multiple specific regex patterns targeting different event types (actions with verbs, disasters, political/military keywords + locations, casualty reports, announcements, specific capitalized names ending in keywords like \"Summit\"). The function collects all matches, filters them for length and common starting words, and ensures uniqueness. The main part iterates through articles, calls this function, aggregates unique results across all articles into a set, and displays a random sample."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygpuenv (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
